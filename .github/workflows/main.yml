name: Java Calculator Test Automation with Zephyr

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up JDK 11
      uses: actions/setup-java@v4
      with:
        java-version: '11'
        distribution: 'temurin'
        cache: maven

    - name: Run Maven Tests
      run: mvn clean test
      continue-on-error: true

    - name: Generate Allure Report
      run: mvn allure:report

    - name: Upload Allure Results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: allure-results
        path: target/allure-results
        retention-days: 30

    - name: Upload Allure Report
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: allure-report
        path: target/site/allure-maven-plugin
        retention-days: 30

    - name: Deploy Allure Report to GitHub Pages
      if: github.ref == 'refs/heads/main' && always()
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: target/site/allure-maven-plugin
        publish_branch: gh-pages

    - name: Install Python Dependencies for Zephyr
      if: always()
      run: |
        pip install requests

    - name: Publish Results to Zephyr Scale
      if: always()
      env:
        ZEPHYR_API_TOKEN: ${{ secrets.ZEPHYR_API_TOKEN }}
        ZEPHYR_PROJECT_KEY: ${{ secrets.ZEPHYR_PROJECT_KEY }}
        GITHUB_SHA: ${{ github.sha }}
        GITHUB_REF_NAME: ${{ github.ref_name }}
        GITHUB_RUN_NUMBER: ${{ github.run_number }}
      run: |
        python3 <<'PYTHON_SCRIPT'
        import xml.etree.ElementTree as ET
        import requests
        import os
        import glob
        from datetime import datetime
        
        # Zephyr Scale Configuration
        API_TOKEN = os.environ.get('ZEPHYR_API_TOKEN')
        PROJECT_KEY = os.environ.get('ZEPHYR_PROJECT_KEY')
        BASE_URL = "https://api.zephyrscale.smartbear.com/v2"
        
        if not API_TOKEN or not PROJECT_KEY:
            print("‚ùå Zephyr credentials not found. Skipping Zephyr integration.")
            exit(0)
        
        headers = {
            "Authorization": f"Bearer {API_TOKEN}",
            "Content-Type": "application/json"
        }
        
        print("=" * 60)
        print("Starting Zephyr Scale Integration")
        print("=" * 60)
        
        # Step 1: Create Test Cycle
        cycle_name = f"GitHub Actions Run #{os.environ.get('GITHUB_RUN_NUMBER')} - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
        cycle_data = {
            "projectKey": PROJECT_KEY,
            "name": cycle_name,
            "description": f"Automated test execution from GitHub Actions\n\nCommit: {os.environ.get('GITHUB_SHA', '')[:7]}\nBranch: {os.environ.get('GITHUB_REF_NAME', 'main')}\nWorkflow Run: #{os.environ.get('GITHUB_RUN_NUMBER', 'N/A')}"
        }
        
        print(f"\nüìù Creating test cycle: {cycle_name}")
        try:
            cycle_response = requests.post(
                f"{BASE_URL}/testcycles",
                json=cycle_data,
                headers=headers,
                timeout=30
            )
            
            if cycle_response.status_code in [200, 201]:
                cycle_key = cycle_response.json().get('key')
                print(f"‚úÖ Test cycle created successfully: {cycle_key}")
            else:
                print(f"‚ùå Failed to create test cycle: {cycle_response.status_code}")
                print(f"Response: {cycle_response.text}")
                exit(1)
        except Exception as e:
            print(f"‚ùå Error creating test cycle: {str(e)}")
            exit(1)
        
        # Step 2: Test Case Mapping (method name -> Zephyr test case key)
        test_mapping = {
            'testAddPositiveNumbers': 'CALC-001',
            'testAddWithNegative': 'CALC-002',
            'testSubtract': 'CALC-003',
            'testMultiply': 'CALC-004',
            'testDivide': 'CALC-005',
            'testDivideByZero': 'CALC-006',
            'testModulo': 'CALC-007',
            'testModuloByZero': 'CALC-008',
            'testPower': 'CALC-009',
            'testSquareRoot': 'CALC-010',
            'testSquareRootNegative': 'CALC-011'
        }
        
        # Step 3: Parse Test Results
        test_files = glob.glob('target/surefire-reports/TEST-*.xml')
        
        if not test_files:
            print("‚ö†Ô∏è No test result files found")
            exit(0)
        
        print(f"\nüìä Found {len(test_files)} test result file(s)")
        
        total_tests = 0
        successful_posts = 0
        failed_posts = 0
        
        for test_file in test_files:
            print(f"\nüìÑ Processing: {test_file}")
            
            try:
                tree = ET.parse(test_file)
                root = tree.getroot()
                
                for testcase in root.findall('testcase'):
                    test_name = testcase.get('name')
                    execution_time = int(float(testcase.get('time', 0)) * 1000)  # Convert to milliseconds
                    
                    # Get Zephyr test case key
                    test_case_key = test_mapping.get(test_name)
                    
                    if not test_case_key:
                        print(f"  ‚ö†Ô∏è No Zephyr mapping for: {test_name}")
                        continue
                    
                    total_tests += 1
                    
                    # Determine test status
                    failure = testcase.find('failure')
                    error = testcase.find('error')
                    skipped = testcase.find('skipped')
                    
                    if failure is not None:
                        status = 'Fail'
                        comment = f"Test Failed\n\nFailure Message:\n{failure.get('message', 'No message')}"
                        if failure.text:
                            comment += f"\n\nStack Trace:\n{failure.text[:300]}"
                    elif error is not None:
                        status = 'Fail'
                        comment = f"Test Error\n\nError Message:\n{error.get('message', 'No message')}"
                        if error.text:
                            comment += f"\n\nStack Trace:\n{error.text[:300]}"
                    elif skipped is not None:
                        status = 'Blocked'
                        comment = 'Test was skipped during execution'
                    else:
                        status = 'Pass'
                        comment = f'Test executed successfully\n\nExecution time: {execution_time}ms'
                    
                    # Limit comment to 1000 characters
                    comment = comment[:1000]
                    
                    # Step 4: Post Test Execution to Zephyr
                    execution_data = {
                        'projectKey': PROJECT_KEY,
                        'testCaseKey': test_case_key,
                        'testCycleKey': cycle_key,
                        'statusName': status,
                        'executionTime': execution_time,
                        'comment': comment
                    }
                    
                    try:
                        exec_response = requests.post(
                            f"{BASE_URL}/testexecutions",
                            json=execution_data,
                            headers=headers,
                            timeout=30
                        )
                        
                        if exec_response.status_code in [200, 201]:
                            successful_posts += 1
                            status_icon = "‚úÖ" if status == "Pass" else "‚ùå" if status == "Fail" else "‚è∏Ô∏è"
                            print(f"  {status_icon} {test_case_key}: {status} ({execution_time}ms)")
                        else:
                            failed_posts += 1
                            print(f"  ‚ùå Failed to post {test_case_key}: HTTP {exec_response.status_code}")
                            print(f"     Response: {exec_response.text[:200]}")
                    
                    except Exception as e:
                        failed_posts += 1
                        print(f"  ‚ùå Error posting {test_case_key}: {str(e)}")
            
            except Exception as e:
                print(f"  ‚ùå Error parsing test file: {str(e)}")
        
        # Step 5: Summary
        print("\n" + "=" * 60)
        print("Zephyr Integration Summary")
        print("=" * 60)
        print(f"Total Tests: {total_tests}")
        print(f"Successfully Posted: {successful_posts}")
        print(f"Failed to Post: {failed_posts}")
        print(f"Test Cycle: {cycle_key}")
        print(f"\nüîó View results in Zephyr Scale:")
        print(f"   https://smartbear.com/test-management")
        print("=" * 60)
        
        if failed_posts > 0:
            print("\n‚ö†Ô∏è Some test results failed to post to Zephyr")
            exit(0)  # Don't fail the workflow
        else:
            print("\n‚úÖ All test results posted successfully!")
        PYTHON_SCRIPT

    - name: Publish Unit Test Results
      if: always()
      uses: EnricoMi/publish-unit-test-result-action@v2
      with:
        files: |
          target/surefire-reports/*.xml

    - name: Comment PR with Test Results
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          let comment = '## üß™ Test Automation Results\n\n';
          
          try {
            const reportFiles = fs.readdirSync('target/surefire-reports')
              .filter(f => f.endsWith('.xml') && f.startsWith('TEST-'));
            
            let totalTests = 0;
            let passedTests = 0;
            let failedTests = 0;
            let skippedTests = 0;
            
            reportFiles.forEach(file => {
              const content = fs.readFileSync(
                path.join('target/surefire-reports', file), 
                'utf8'
              );
              
              const testsMatch = content.match(/tests="(\d+)"/);
              const failuresMatch = content.match(/failures="(\d+)"/);
              const skippedMatch = content.match(/skipped="(\d+)"/);
              
              if (testsMatch) totalTests += parseInt(testsMatch[1]);
              if (failuresMatch) failedTests += parseInt(failuresMatch[1]);
              if (skippedMatch) skippedTests += parseInt(skippedMatch[1]);
            });
            
            passedTests = totalTests - failedTests - skippedTests;
            
            const passRate = totalTests > 0 ? ((passedTests / totalTests) * 100).toFixed(1) : 0;
            
            comment += '### üìä Summary\n\n';
            comment += `| Metric | Count |\n`;
            comment += `|--------|-------|\n`;
            comment += `| ‚úÖ Passed | ${passedTests} |\n`;
            comment += `| ‚ùå Failed | ${failedTests} |\n`;
            comment += `| ‚è∏Ô∏è Skipped | ${skippedTests} |\n`;
            comment += `| üìà Total | ${totalTests} |\n`;
            comment += `| üéØ Pass Rate | ${passRate}% |\n\n`;
            
            if (failedTests > 0) {
              comment += '### ‚ùå Failed Tests\n\n';
              comment += 'Please check the workflow logs for details.\n\n';
            }
            
            comment += '### üìÅ Artifacts\n\n';
            comment += `- [Download Allure Results](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})\n`;
            comment += `- [View Test Reports](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})\n\n`;
            
            comment += '### üîó Zephyr Scale\n\n';
            comment += 'Test results have been published to Zephyr Scale. Check your Zephyr dashboard for detailed execution results.\n\n';
            
            comment += `---\n*Automated by GitHub Actions - Run #${{ github.run_number }}*`;
            
          } catch (error) {
            comment += '‚ö†Ô∏è Unable to parse test results\n';
            comment += `Error: ${error.message}\n`;
          }
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
